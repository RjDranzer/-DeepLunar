{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e45e6f-78eb-4ac9-a4b9-e321dda67fa2",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "832d0e2d-6f71-420b-bd3a-9640c03d542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ea276-32e4-4975-b0f8-39c09dd3f9ad",
   "metadata": {},
   "source": [
    "# Setup - for Depth Anything Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60ad45c0-8694-4a93-b57f-1c0bc5194423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - for Depth Anything Library\n",
    "repo_path = r\"D:\\project\\ISRO_final\\Test tube detection\\Depth-Anything\"\n",
    "\n",
    "repo_dir = Path(repo_path)\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/LiheYoung/Depth-Anything\"], check=True)\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"openvino>=2023.3.0\", \"datasets>=2.14.6\", \"nncf\"], check=True)\n",
    "\n",
    "\n",
    "\n",
    "# setup_attention_file():\n",
    "atten_path = r\"D:\\project\\ISRO_final\\Test tube detection\\Depth-Anything\\torchhub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py\"\n",
    "attention_file_path = Path(atten_path)\n",
    "orig_attention_path = attention_file_path.parent / (\"orig_\" + attention_file_path.name)\n",
    "\n",
    "if not orig_attention_path.exists():\n",
    "    attention_file_path.rename(orig_attention_path)\n",
    "\n",
    "    with orig_attention_path.open(\"r\") as f:\n",
    "        data = f.read()\n",
    "        data = data.replace(\"XFORMERS_AVAILABLE = True\", \"XFORMERS_AVAILABLE = False\")\n",
    "        with attention_file_path.open(\"w\") as out_f:\n",
    "            out_f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7e3b0-c1f9-4fc2-8d98-4b20c59c1883",
   "metadata": {},
   "source": [
    "# Histogram Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49637335-4134-48b2-a3f6-1b5bcb80c364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27ceb39-9bb9-44e1-a8ee-0604d2930c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def hist_equal(image):\n",
    "    lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8, 8))\n",
    "    clahe_img = clahe.apply(l)\n",
    "\n",
    "    clahe_upd = cv2.merge((clahe_img, a, b))\n",
    "    clahe_bgr = cv2.cvtColor(clahe_upd, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return clahe_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a4b78-8222-48c1-9b0e-d32e0426c58c",
   "metadata": {},
   "source": [
    "# HSV based detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2f38bd9-34ed-4793-bba3-b4c6644f5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_detect(image):\n",
    "    \"\"\"Detects red color regions in an image using HSV color space.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Define red color range\n",
    "    low_red = np.array([0, 150, 150])\n",
    "    high_red = np.array([10, 255, 255])\n",
    "\n",
    "    # Create a mask for red color\n",
    "    mask = cv2.inRange(hsv_img, low_red, high_red)\n",
    "\n",
    "    # Apply morphological operations for noise reduction (optional)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    dilation = cv2.dilate(mask, kernel)\n",
    "    img_median_mask = cv2.medianBlur(dilation, 9)\n",
    "    mask = img_median_mask\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "\n",
    "    return result, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb94a81-df77-438d-adbf-f7feceeb28b3",
   "metadata": {},
   "source": [
    "# Frame Stabilizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dba3017-f4f5-41b7-833a-27c6990575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilize_frame(frame):\n",
    "    class Tracker:\n",
    "        def __init__(self):\n",
    "            self.trackedFeatures = []\n",
    "            self.prevGray = None\n",
    "            self.freshStart = True\n",
    "            self.rigidTransform = np.eye(3, dtype=np.float32)  # Affine 2x3 in a 3x3 matrix\n",
    "\n",
    "        def process_image(self, img):\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if self.freshStart:\n",
    "                corners = cv2.goodFeaturesToTrack(gray, 300, 0.01, 10)\n",
    "                if corners is not None:\n",
    "                    corners = corners.reshape(-1, 2)\n",
    "                    self.trackedFeatures.extend(corners)\n",
    "                self.freshStart = False\n",
    "\n",
    "            if self.prevGray is not None:\n",
    "                status = np.empty((len(self.trackedFeatures), 1), dtype=np.uint8)\n",
    "                errors = np.empty_like(status)\n",
    "                corners, status, _ = cv2.calcOpticalFlowPyrLK(self.prevGray, gray, np.array(self.trackedFeatures),\n",
    "                                                               None, status, errors, winSize=(10, 10))\n",
    "\n",
    "                if len(status) - np.count_nonzero(status) > 0.2 * len(status):\n",
    "                    self.rigidTransform = np.eye(3, dtype=np.float32)\n",
    "                    self.trackedFeatures.clear()\n",
    "                    self.prevGray = None\n",
    "                    self.freshStart = True\n",
    "                    return\n",
    "                else:\n",
    "                    self.freshStart = False\n",
    "\n",
    "                new_rigid_transform = cv2.estimateAffinePartial2D(np.array(self.trackedFeatures), corners)[0]\n",
    "                if new_rigid_transform is not None:\n",
    "                    new_rigid_transform = np.vstack([new_rigid_transform, [0, 0, 1]])\n",
    "                    self.rigidTransform = np.dot(new_rigid_transform, self.rigidTransform)\n",
    "\n",
    "                self.trackedFeatures = [corner for corner, stat in zip(corners, status) if stat]\n",
    "\n",
    "            self.prevGray = gray.copy()\n",
    "\n",
    "        def stabilize_frame(self, frame):\n",
    "            self.process_image(frame)\n",
    "\n",
    "            inv_trans = np.linalg.inv(self.rigidTransform)[:2]\n",
    "            stabilized_frame = cv2.warpAffine(frame, inv_trans, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            return stabilized_frame\n",
    "\n",
    "    tracker = Tracker()\n",
    "    stabilized_frame = tracker.stabilize_frame(frame)\n",
    "    return stabilized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3940f0-187f-4e5e-9ed3-af00859ee954",
   "metadata": {},
   "source": [
    "# Moment Based Orientation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7def32c-8aee-4201-be36-a3a583b13597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_orientation(frame_or_mask):\n",
    "    # Convert the input frame or mask to grayscale\n",
    "    gray = cv2.cvtColor(frame_or_mask, cv2.COLOR_BGR2GRAY) if len(frame_or_mask.shape) == 3 else frame_or_mask\n",
    "\n",
    "    # Calculate moments\n",
    "    moments = cv2.moments(gray)\n",
    "\n",
    "    # Calculate area\n",
    "    area = moments['m00']\n",
    "\n",
    "    # Calculate centroid\n",
    "    centroid_x = int(moments['m10'] / moments['m00']) if area != 0 else 0\n",
    "    centroid_y = int(moments['m01'] / moments['m00']) if area != 0 else 0\n",
    "\n",
    "    # Calculate orientation\n",
    "    mu20 = moments['mu20']\n",
    "    mu02 = moments['mu02']\n",
    "    mu11 = moments['mu11']\n",
    "\n",
    "    orientation_rad = 0.5 * np.arctan2(2 * mu11, mu20 - mu02)\n",
    "    orientation_deg = np.degrees(orientation_rad)\n",
    "\n",
    "    return area, (centroid_x, centroid_y), orientation_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3b42f-c6c4-4e66-8df2-b607bc136c96",
   "metadata": {},
   "source": [
    "# Depth Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da2eb576-9100-4d7b-ab86-83d68b174493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "def initialize_model(encoder='vit'):\n",
    "    model_id = f'depth_anything_{encoder}14'\n",
    "    depth_anything = DepthAnything.from_pretrained(f'LiheYoung/{model_id}')\n",
    "    return depth_anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc09622d-f6b7-4261-8b28-91f249a50fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model_id, depth_anything, example_input):\n",
    "    OV_DEPTH_ANYTHING_PATH = Path(f'{model_id}.xml') \n",
    "    \n",
    "    if not OV_DEPTH_ANYTHING_PATH.exists():\n",
    "        ov_model = ov.convert_model(depth_anything, example_input=example_input, input=[1, 3, 518, 518])\n",
    "        ov.save_model(ov_model, OV_DEPTH_ANYTHING_PATH)\n",
    "        \n",
    "    core = ov.Core()\n",
    "    \n",
    "    device = widgets.Dropdown(\n",
    "        options=core.available_devices + [\"AUTO\"],\n",
    "        value=\"AUTO\",\n",
    "        description=\"Device:\",\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    compiled_model = core.compile_model(OV_DEPTH_ANYTHING_PATH, device.value)\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09f14a3c-7923-4a8d-b25e-849667f9fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    transform = Compose([\n",
    "        Resize(\n",
    "            width=518,\n",
    "            height=518,\n",
    "            resize_target=False,\n",
    "            ensure_multiple_of=14,\n",
    "            resize_method='lower_bound',\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        PrepareForNet(),\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83474a78-9e22-45a6-a4c9-8e394a83f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(data):\n",
    "    \"\"\"Normalizes the values in `data` between 0 and 1\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b5e2b27-fe30-4923-aa39-ef7378da19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(result, colormap=\"viridis\"):\n",
    "    \"\"\"\n",
    "    Convert network result of floating point numbers to an RGB image with\n",
    "    integer values from 0-255 by applying a colormap.\n",
    "\n",
    "    `result` is expected to be a single network result in 1,H,W shape\n",
    "    `colormap` is a matplotlib colormap.\n",
    "    See https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    \"\"\"\n",
    "    result = result.squeeze(0)\n",
    "    result = normalize_minmax(result)\n",
    "    result = result * 255\n",
    "    result = result.astype(np.uint8)\n",
    "    depth_map = cv2.applyColorMap(result, cv2.COLORMAP_INFERNO)[:, :, ::-1]\n",
    "    return depth_map, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e8012d-4ec2-475c-8c78-4d80a9f86fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(image_data):\n",
    "    \"\"\"\n",
    "    Convert image_data from BGR to RGB\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aabd5005-adcd-45e9-9899-fefabb67d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_anything_model():\n",
    "    encoder = 'vits'\n",
    "    model_id = f'depth_anything_{encoder}14'\n",
    "    depth_anything = initialize_model(encoder)\n",
    "    example_input = np.random.rand(1, 3, 518, 518).astype(np.float32)\n",
    "    compiled_model = compile_model(model_id, depth_anything, example_input)\n",
    "    \n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1963f-e0ad-4112-a991-bfc78d99ed53",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bbb3ec6-b3b8-453c-b279-2b86e27ea581",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Open the video file or capture device\n",
    "    video_path = 0\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"The video cannot be opened.\")\n",
    "\n",
    "    transform = get_transform()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Desired dimensions for each frame\n",
    "            display_h, display_w = 200, 400\n",
    "\n",
    "            # Resize frame to the display dimensions\n",
    "            frame_resized = cv2.resize(frame, (display_w, display_h))\n",
    "\n",
    "            # Stabilize frame\n",
    "            stable_frame = stabilize_frame(frame_resized)\n",
    "\n",
    "            # Histogram Equalization\n",
    "            histframe = hist_equal(stable_frame)\n",
    "\n",
    "            # Run HSV detection asynchronously\n",
    "            masked_img, mask = hsv_detect(frame_resized)\n",
    "            \n",
    "            # Convert results to BGR for display\n",
    "            result_bgr = cv2.cvtColor(masked_img, cv2.COLOR_RGB2BGR)\n",
    "            mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Run depth model inference asynchronously\n",
    "            compiled_model = depth_anything_model()\n",
    "            input_image = cv2.cvtColor(histframe, cv2.COLOR_BGR2RGB) / 255.0\n",
    "            input_image = transform({'image': input_image})['image']\n",
    "            input_image = np.expand_dims(input_image, 0)\n",
    "            result = compiled_model([input_image])[0]\n",
    "            \n",
    "            # Convert network result to RGB image\n",
    "            result_frame, pixel_depth = convert_result_to_image(result)\n",
    "\n",
    "            # Resize result frame to match the display dimensions\n",
    "            result_frame_resized = cv2.resize(result_frame, (display_w, display_h))\n",
    "\n",
    "            #cv2.cvtColor(result_frame_resized, )\n",
    "            result_xyz = cv2.bitwise_and(result_frame_resized, result_frame_resized, mask=mask)\n",
    "\n",
    "            # Ensure all images have the same size\n",
    "            mask_bgr = cv2.resize(mask_bgr, (display_w, display_h))\n",
    "            result_bgr = cv2.resize(result_bgr, (display_w, display_h))\n",
    "            result_xyz = cv2.resize(result_xyz, (display_w, display_h))\n",
    "\n",
    "            # Stack images for better visualization\n",
    "            h_stack1 = np.hstack((frame_resized, mask_bgr))\n",
    "            h_stack2 = np.hstack((result_bgr, result_xyz))\n",
    "\n",
    "            # Combine both stacks vertically\n",
    "            final_display = np.vstack((h_stack1, h_stack2))\n",
    "\n",
    "            # Display the stacked images in a single window\n",
    "            cv2.imshow(\"Combined Display\", final_display)\n",
    "            \n",
    "            # Exit on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Processing interrupted.\")\n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42031192-2f2c-42a6-9643-5f082b3f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main coroutine\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d7c87-5c96-480d-87c9-19d96ea32f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90df518-0a45-43fe-98fb-e89c0e1df162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f1be5-582b-4c23-81ec-3195152be899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb378ac-d4ee-43ca-bc50-4fe8aa37b187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a362f8-62b7-4ee1-9c61-3f4f0fa8fb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a6b72-9659-493d-8a0b-344c6825e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa6849-f821-4df6-afb7-b9ecf4471f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03ea94-b3b1-4810-b263-6b0f35617c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608c2bc-b577-4b41-bf8d-d284c42e8770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bf33c-4218-4c02-9673-fda1ddd72664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5abe1313-848b-4487-b065-8b0c131c1be7",
   "metadata": {},
   "source": [
    "# Trial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a522f514-4188-404c-a9cc-cf6450146d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'async def compile_model_async(model_id, depth_anything, example_input):\\n    OV_DEPTH_ANYTHING_PATH = Path(f\\'{model_id}.xml\\') \\n    \\n    if not OV_DEPTH_ANYTHING_PATH.exists():\\n        ov_model = ov.convert_model(depth_anything, example_input=example_input, input=[1, 3, 518, 518])\\n        ov.save_model(ov_model, OV_DEPTH_ANYTHING_PATH)\\n        \\n    core = ov.Core()\\n    \\n    device = widgets.Dropdown(\\n        options=core.available_devices + [\"AUTO\"],\\n        value=\"AUTO\",\\n        description=\"Device:\",\\n        disabled=False,\\n    )\\n\\n    compiled_model = core.compile_model(OV_DEPTH_ANYTHING_PATH, device.value)\\n    return compiled_model\\n\\nasync def depth_anything_model():\\n    encoder = \\'vits\\'\\n    model_id = f\\'depth_anything_{encoder}14\\'\\n    depth_anything = initialize_model(encoder)\\n    example_input = np.random.rand(1, 3, 518, 518).astype(np.float32)\\n    compiled_model = await compile_model_async(model_id, depth_anything, example_input)\\n    \\n    return compiled_model\\n\\nasync def main():\\n    # Open the video file or capture device\\n    video_path = 0\\n    cap = cv2.VideoCapture(video_path)\\n\\n    if not cap.isOpened():\\n        raise ValueError(\"The video cannot be opened.\")\\n\\n    transform = get_transform()\\n\\n    try:\\n        while True:\\n            ret, frame = cap.read()\\n            if not ret:\\n                break\\n\\n            # Desired dimensions for each frame\\n            display_h, display_w = 200, 400\\n\\n            # Resize frame to the display dimensions\\n            frame_resized = cv2.resize(frame, (display_w, display_h))\\n\\n            # Stabilize frame\\n            stable_frame = stabilize_frame(frame_resized)\\n\\n            # Histogram Equalization\\n            histframe = hist_equal(stable_frame)\\n\\n            # Run HSV detection asynchronously\\n            masked_img, mask = hsv_detect(frame_resized)\\n            \\n            # Convert results to BGR for display\\n            result_bgr = cv2.cvtColor(masked_img, cv2.COLOR_RGB2BGR)\\n            mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\\n\\n            # Run depth model inference asynchronously\\n            compiled_model = await depth_anything_model()\\n            input_image = cv2.cvtColor(histframe, cv2.COLOR_BGR2RGB) / 255.0\\n            input_image = transform({\\'image\\': input_image})[\\'image\\']\\n            input_image = np.expand_dims(input_image, 0)\\n            result = compiled_model([input_image])[0]\\n            \\n            # Convert network result to RGB image\\n            result_frame, pixel_depth = convert_result_to_image(result)\\n\\n            # Resize result frame to match the display dimensions\\n            result_frame_resized = cv2.resize(result_frame, (display_w, display_h))\\n\\n            #cv2.cvtColor(result_frame_resized, )\\n            result_xyz = cv2.bitwise_and(result_frame_resized, result_frame_resized, mask=mask)\\n\\n            # Display the resulting frames\\n            cv2.imshow(\\'Original Frame\\', frame_resized)\\n            cv2.imshow(\\'Mask\\', mask_bgr)\\n            cv2.imshow(\\'Masked Image\\', result_bgr)\\n            #cv2.imshow(\\'Depth Image\\', result_frame_resized)\\n            cv2.imshow(\\'Depth Image\\', result_xyz)\\n\\n            # Exit on \\'q\\' key press\\n            if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n                break\\n\\n    except KeyboardInterrupt:\\n        print(\"Processing interrupted.\")\\n    finally:\\n        # Release resources\\n        cap.release()\\n        cv2.destroyAllWindows()\\n\\n# Run the main coroutine\\nawait main()'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''async def compile_model_async(model_id, depth_anything, example_input):\n",
    "    OV_DEPTH_ANYTHING_PATH = Path(f'{model_id}.xml') \n",
    "    \n",
    "    if not OV_DEPTH_ANYTHING_PATH.exists():\n",
    "        ov_model = ov.convert_model(depth_anything, example_input=example_input, input=[1, 3, 518, 518])\n",
    "        ov.save_model(ov_model, OV_DEPTH_ANYTHING_PATH)\n",
    "        \n",
    "    core = ov.Core()\n",
    "    \n",
    "    device = widgets.Dropdown(\n",
    "        options=core.available_devices + [\"AUTO\"],\n",
    "        value=\"AUTO\",\n",
    "        description=\"Device:\",\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    compiled_model = core.compile_model(OV_DEPTH_ANYTHING_PATH, device.value)\n",
    "    return compiled_model\n",
    "\n",
    "    \n",
    "\n",
    "async def depth_anything_model():\n",
    "    encoder = 'vits'\n",
    "    model_id = f'depth_anything_{encoder}14'\n",
    "    depth_anything = initialize_model(encoder)\n",
    "    example_input = np.random.rand(1, 3, 518, 518).astype(np.float32)\n",
    "    compiled_model = await compile_model_async(model_id, depth_anything, example_input)\n",
    "    \n",
    "    return compiled_model\n",
    "\n",
    "    \n",
    "\n",
    "async def main():\n",
    "    # Open the video file or capture device\n",
    "    video_path = 0\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"The video cannot be opened.\")\n",
    "\n",
    "    transform = get_transform()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Desired dimensions for each frame\n",
    "            display_h, display_w = 200, 400\n",
    "\n",
    "            # Resize frame to the display dimensions\n",
    "            frame_resized = cv2.resize(frame, (display_w, display_h))\n",
    "\n",
    "            # Stabilize frame\n",
    "            stable_frame = stabilize_frame(frame_resized)\n",
    "\n",
    "            # Histogram Equalization\n",
    "            histframe = hist_equal(stable_frame)\n",
    "\n",
    "            # Run HSV detection asynchronously\n",
    "            masked_img, mask = hsv_detect(frame_resized)\n",
    "            \n",
    "            # Convert results to BGR for display\n",
    "            result_bgr = cv2.cvtColor(masked_img, cv2.COLOR_RGB2BGR)\n",
    "            mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Run depth model inference asynchronously\n",
    "            compiled_model = await depth_anything_model()\n",
    "            input_image = cv2.cvtColor(histframe, cv2.COLOR_BGR2RGB) / 255.0\n",
    "            input_image = transform({'image': input_image})['image']\n",
    "            input_image = np.expand_dims(input_image, 0)\n",
    "            result = compiled_model([input_image])[0]\n",
    "            \n",
    "            # Convert network result to RGB image\n",
    "            result_frame, pixel_depth = convert_result_to_image(result)\n",
    "\n",
    "            # Resize result frame to match the display dimensions\n",
    "            result_frame_resized = cv2.resize(result_frame, (display_w, display_h))\n",
    "\n",
    "            #cv2.cvtColor(result_frame_resized, )\n",
    "            result_xyz = cv2.bitwise_and(result_frame_resized, result_frame_resized, mask=mask)\n",
    "\n",
    "            # Display the resulting frames\n",
    "            cv2.imshow('Original Frame', frame_resized)\n",
    "            cv2.imshow('Mask', mask_bgr)\n",
    "            cv2.imshow('Masked Image', result_bgr)\n",
    "            #cv2.imshow('Depth Image', result_frame_resized)\n",
    "            cv2.imshow('Depth Image', result_xyz)\n",
    "\n",
    "            # Exit on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Processing interrupted.\")\n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main coroutine\n",
    "await main()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba345f0c-d49f-4041-814b-f2decc02bda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def hsv_detect(image):\\n\\n    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n    \\n    # Convert the image to HSV\\n    hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\\n\\n    # Red Color range\\n    low_red = np.array([0, 150, 150])\\n    high_red = np.array([10, 255, 255])\\n\\n    # Create a mask for the red color\\n    mask = cv2.inRange(hsv_img, low_red, high_red)\\n\\n    # Morphological transform\\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\\n    dilation = cv2.dilate(mask,kernel)\\n\\n    # Binarization\\n    img_median_mask = cv2.medianBlur(dilation, 9)\\n\\n    gray_img = cv2.cvtColor(visualization_image, cv2.COLOR_RGB2GRAY)\\n    otsu_value , result_img = cv2.threshold(gray_img,0,255, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)\\n\\n    # Apply the mask to the original image\\n    result = cv2.bitwise_and(img_rgb, img_rgb, mask=img_median_mask)\\n\\n    return result, img_median_mask '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def hsv_detect(image):\n",
    "\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert the image to HSV\n",
    "    hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Red Color range\n",
    "    low_red = np.array([0, 150, 150])\n",
    "    high_red = np.array([10, 255, 255])\n",
    "\n",
    "    # Create a mask for the red color\n",
    "    mask = cv2.inRange(hsv_img, low_red, high_red)\n",
    "\n",
    "    # Morphological transform\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    dilation = cv2.dilate(mask,kernel)\n",
    "\n",
    "    # Binarization\n",
    "    img_median_mask = cv2.medianBlur(dilation, 9)\n",
    "\n",
    "    gray_img = cv2.cvtColor(visualization_image, cv2.COLOR_RGB2GRAY)\n",
    "    otsu_value , result_img = cv2.threshold(gray_img,0,255, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=img_median_mask)\n",
    "\n",
    "    return result, img_median_mask '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e74f550-e8cf-42e4-b492-e2278076e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef hist_equal(image):\\n    \\n    lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\\n    l, a, b = cv2.split(lab_img)\\n    \\n    clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8,8))\\n    clahe_img = clahe.apply(l)\\n\\n    clahe_upd = cv2.merge((clahe_img,a,b))\\n    clahe = cv2.cvtColor(clahe_upd, cv2.COLOR_LAB2BGR)\\n\\n    final_img = cv2.cvtColor(clahe, cv2.COLOR_BGR2RGB)\\n\\n    return final_img '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def hist_equal(image):\n",
    "    \n",
    "    lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8,8))\n",
    "    clahe_img = clahe.apply(l)\n",
    "\n",
    "    clahe_upd = cv2.merge((clahe_img,a,b))\n",
    "    clahe = cv2.cvtColor(clahe_upd, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    final_img = cv2.cvtColor(clahe, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return final_img '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfd7a0-d2fa-4c43-87cf-cacbce869fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
